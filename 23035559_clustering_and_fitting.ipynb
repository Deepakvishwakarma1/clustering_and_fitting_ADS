# Assignment for CLUSTERING AND FITTING

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from scipy.stats import skew, kurtosis
import warnings
warnings.filterwarnings("ignore")

## Reading The Data

df = pd.read_csv('world-happiness-report.csv')

df.head()

df.columns

df.info()

df.describe().T

df.isna().sum()

df = df.dropna()

df.shape

## Statistical Analysis

df.corr()

stats_df = df.describe().loc[['mean', 'std']]
skewness = df.skew()
kurt = df.kurtosis()
stats_df.loc['skewness'] = skewness
stats_df.loc['kurtosis'] = kurt
stats_df = stats_df.T
columns = {'mean':'Mean', 'std' : 'Standard Deviation', 'skewness' : 'Skewness', 'kurtosis': 'Kurtosis'}
stats_df = stats_df.rename(columns = columns)
stats_df

## Graphical Analysis 

def plot_top_five_countries_pie(data):
    """
    Creates a pie chart showing the distribution of Life Ladder scores among the top five countries.

    Parameters:
    - data (DataFrame): DataFrame containing the data with columns 'Country name' and 'Life Ladder'.

    Returns:
    - None
    """
    # Sort the data by Life Ladder scores
    sorted_data = data.sort_values(by='Life Ladder', ascending=False)

    # Select the top five countries
    top_five = sorted_data.head(5)

    # Plotting
    plt.figure(figsize=(8, 8))
    plt.pie(top_five['Life Ladder'], labels=top_five['Country name'], autopct='%1.1f%%', startangle=140)
    plt.title('Distribution of Life Ladder Scores for Top Five Countries')
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.tight_layout()
    plt.show()

plot_top_five_countries_pie(df)

def heatmap(dataframe):
    """
    Visualize the correlation matrix of a DataFrame using a heatmap.

    Parameters:
    dataframe (DataFrame): The DataFrame containing the data.

    Returns:
    None
    """
    plt.figure(figsize=(10, 5))
    sns.heatmap(dataframe.corr(), annot=True)
    plt.title("Correlation Matrix Heatmap")
    plt.show()

heatmap(df)

## Fitting

def fit_linear_regression(data):
    """
    Fit a linear regression model to analyze the relationship between
    a dependent variable and an independent variable with train-test split.

    Parameters:
    - dataset (DataFrame): The dataset containing the variables.
    - independent_var (str): The name of the independent variable.
    - dependent_var (str): The name of the dependent variable.
    - test_size (float): The proportion of the dataset to include in the test split.

    Returns:
    - model (LinearRegression): Fitted linear regression model.
    - X_test (DataFrame): Independent variable values for the test data.
    - y_test (Series): Actual test values of the dependent variable.
    """

    # Split the dataset into training and testing sets
    X = data[independent_var]
    y = data[dependent_var]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.02, random_state=42)
    
    # Reshape X_train and X_test to 2D arrays
    X_train = X_train.values.reshape(-1, 1)
    X_test = X_test.values.reshape(-1, 1)
    
    model = LinearRegression()
    model.fit(X_train, y_train)
    
    return model, X_test, y_test

independent_var = 'Healthy life expectancy at birth' 
dependent_var = 'Life Ladder'  
# Fit linear regression model with train-test split for alchohol and quality
model, X_test, y_test = fit_linear_regression(df)

# Evaluate model performance
predictions = model.predict(X_test)
# Fit linear regression model with train-test split for alchohol and quality


predictions = model.predict(X_test)
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)
print("Model - Mean Squared Error:", mse)
print("Model - R-squared Score:", r2)

confidence = 0.95
n = len(X_test)
mean_x = np.mean(X_test)
t_value = 2.262  # for 95% confidence interval with n-2 degrees of freedom
se = np.sqrt(np.sum((y_test - predictions)**2) / (n - 2))
margin_of_error = t_value * se
lower_bound = predictions - margin_of_error
upper_bound = predictions + margin_of_error


def plot_regression_results(X_test, y_test, predictions, margin_of_error):
    """
    Plot the results of a regression model, including actual vs. predicted values with confidence intervals.

    Parameters:
        X_test (array-like): The feature values for testing.
        y_test (array-like): The actual target values for testing.
        predictions (array-like): The predicted target values.
        margin_of_error (array-like): The margin of error for each prediction.

    Returns:
        None
    """
    # Convert arrays to numpy arrays if not already
    X_test = np.array(X_test)
    y_test = np.array(y_test)
    predictions = np.array(predictions)
    margin_of_error = np.array(margin_of_error)

    # Plotting
    plt.figure(figsize=(10, 6))
    plt.scatter(X_test[:, 0], y_test, color='green', label='Actual (CO(GT))', marker='o')  # Assuming X_test has multiple features
    plt.plot(X_test[:, 0], predictions, color='red', label='Predicted (CO(GT))', marker='x')
    plt.errorbar(X_test[:, 0], predictions, yerr=margin_of_error, fmt='o', color='blue', label='95% Confidence Intervals', capsize=4, elinewidth=0.5, capthick=0.5)
    plt.xlabel('Healthy life expectancy at birth')  # Assuming X_test has multiple features
    plt.ylabel('Life Ladder')
    plt.title('Fitted Regression Line with Confidence Intervals')
    plt.legend()
    plt.grid(True)

    plt.show()


plot_regression_results(X_test, y_test, predictions, margin_of_error)

## Clustering

# Correlation Analysis
numeric_data = df.drop('Country name', axis = 1)


# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)

correlation_matrix = numeric_data.corr()
# Select variables with high correlation coefficient (> 0.5 or <-0.5)
high_corr_vars = correlation_matrix[(correlation_matrix > 0.5) | (correlation_matrix < -0.5)]

print("Highly correlated variables:")
print(high_corr_vars)

# Dimensionality Reduction using PCA
pca = PCA(n_components=2)  # Set the number of components as per your requirement
pca.fit(scaled_data)
pca_components = pca.components_
print("PCA:")
print(pca_components)

selected_columns = ['Log GDP per capita', 'Social support','Healthy life expectancy at birth', 'Freedom to make life choices']
data = df[selected_columns]

# Normalize and back-scale the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# Use the elbow method to find the optimal number of clusters
inertia = []
silhouette_scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    clusters = kmeans.fit_predict(scaled_data)
    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(scaled_data, clusters))

# Based on the elbow curve or silhouette score, choose the optimal number of clusters
k = silhouette_scores.index(max(silhouette_scores)) + 2  # Add 2 to account for starting at 2 clusters


def plot_elbow_curve_and_silhouette_scores(inertia, silhouette_scores):
    """
    Plot the elbow curve and silhouette scores to find the optimal number of clusters.

    Parameters:
        inertia (list or array-like): List of inertia values for different numbers of clusters.
        silhouette_scores (list or array-like): List of silhouette scores for different numbers of clusters.

    Returns:
        None

    This function plots the elbow curve and silhouette scores to help determine the optimal number of clusters
    for a clustering algorithm. The elbow curve shows the inertia (within-cluster sum of squares) for different
    numbers of clusters, while the silhouette scores measure the compactness and separation of clusters.
    The optimal number of clusters is often identified as the point where the inertia starts to decrease
    at a slower rate (the "elbow" point) and the silhouette score is highest.
    """
    # Create a figure and axis
    fig, ax1 = plt.subplots(figsize=(10, 6))

    # Plot inertia (elbow curve)
    color = 'tab:blue'
    ax1.set_xlabel('Number of Clusters')
    ax1.set_ylabel('Inertia', color=color)
    ax1.plot(range(2, 11), inertia, marker='o', linestyle='-', color=color)
    ax1.tick_params(axis='y', labelcolor=color)

    # Set plot title, ticks, and grid
    plt.title('Elbow Method for Optimal K')
    plt.xticks(range(2, 11))
    plt.grid(True)

    # Show the plot
    plt.show()



plot_elbow_curve_and_silhouette_scores(inertia, silhouette_scores)


def clustering(data, num_clusters):
    """
    Perform K-means Clustering on the given data.

    Parameters:
    - data (array-like): The input data for clustering.
    - num_clusters (int): The number of clusters to form.

    Returns:
    - clusters (array): Array of cluster labels for each data point.
    - cluster_centers (array): Array of cluster centers in the original feature space.
    """
    # Standardize the data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data)
    
    # Apply K-means Clustering
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    clusters = kmeans.fit_predict(scaled_data)
    
    # Get cluster centers
    cluster_centers = kmeans.cluster_centers_
    
    # Inverse transform cluster centers to original feature space
    cluster_centers = scaler.inverse_transform(cluster_centers)
    
    return clusters, cluster_centers

def visualize(data, clusters, cluster_centers, kmeans):
    """
    Visualize the clusters along with predicted points.

    Parameters:
    - data (array-like): The original data.
    - clusters (array): Array of cluster labels for each data point.
    - cluster_centers (array): Array of cluster centers in the original feature space.
    - Agglomerative : Fitted KMeans model.
    """
    # Visualize original data points
    plt.figure(figsize=(10, 6))
    for cluster in np.unique(clusters):
        cluster_data = data[clusters == cluster]
        plt.scatter(cluster_data[:, 1], cluster_data[:, 0], label=f'Cluster {cluster+1}', alpha=0.7)
    plt.scatter(cluster_centers[:, 1], cluster_centers[:, 0], marker='o', color='red', label='Cluster Centers')
    # Predict and visualize additional points
    for center in cluster_centers:
        # Generate multiple random points around each cluster center
        for _ in range(5):
            random_index = np.random.randint(len(data))
            random_point = data[random_index] + np.random.randn(1, data.shape[1]) * 0.1
            cluster_label = kmeans.predict(random_point)[0]
            plt.scatter(random_point[:, 1], random_point[:, 0], marker='x', color='black')
    
    plt.xlabel('Log GDP per capita')
    plt.ylabel('Social support')
    plt.title('Clusters with Predicted Points')
    plt.legend()
    plt.grid(True)
    plt.savefig('cluster')
    plt.show()
    




clusters, cluster_centers = clustering(data, k)
visualize(data.values, clusters, cluster_centers, kmeans)

kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans.fit_predict(scaled_data)
silhouette_avg = silhouette_score(scaled_data, clusters)
print("Silhouette Score:", silhouette_avg)


# Guided By:-Prof. William Cooper

# Submitted By:- Deepak Vishwakarma

# Student ID:- 23035559

